<documents>
<document index='0'>
<source>https://python.langchain.com/docs/modules/model_io/llms/streaming_llm</source>
<doc_content>Streaming | ğŸ¦œï¸ğŸ”— Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# Streaming

All `LLM`s implement the `Runnable` interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all `LLM`s basic support for streaming.

Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying `LLM` provider. This obviously doesn't give you token-by-token streaming, which requires native support from the `LLM` provider, but ensures your code that expects an iterator of tokens can work for any of our `LLM` integrations.

See which [integrations support token-by-token streaming here](/docs/integrations/llms/).

```python
from langchain.llms import OpenAI

llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0, max_tokens=512)
for chunk in llm.stream("Write me a song about sparkling water."):
    print(chunk, end="", flush=True)
```

```text
    
    
    Verse 1:
    Bubbles dancing in my glass
    Clear and crisp, it's such a blast
    Refreshing taste, it's like a dream
    Sparkling water, you make me beam
    
    Chorus:
    Oh sparkling water, you're my delight
    With every sip, you make me feel so right
    You're like a party in my mouth
    I can't get enough, I'm hooked no doubt
    
    Verse 2:
    No sugar, no calories, just pure bliss
    You're the perfect drink, I must confess
    From lemon to lime, so many flavors to choose
    Sparkling water, you never fail to amuse
    
    Chorus:
    Oh sparkling water, you're my delight
    With every sip, you make me feel so right
    You're like a party in my mouth
    I can't get enough, I'm hooked no doubt
    
    Bridge:
    Some may say you're just plain water
    But to me, you're so much more
    You bring a sparkle to my day
    In every single way
    
    Chorus:
    Oh sparkling water, you're my delight
    With every sip, you make me feel so right
    You're like a party in my mouth
    I can't get enough, I'm hooked no doubt
    
    Outro:
    So here's to you, my dear sparkling water
    You'll always be my go-to drink forever
    With your effervescence and refreshing taste
    You'll always have a special place.
```</doc_content>
</document>





<document index='1'>
<source>https://api.python.langchain.com/en/latest/llms/langchain.llms.petals.Petals.html</source>
<doc_content>langchain.llms.petals.Petals â€” ğŸ¦œğŸ”— LangChain 0.0.337

API

Experimental

Python Docs

Toggle Menu

PrevUp
Next

LangChain 0.0.337

langchain.llms.petals.Petals

langchain.llms.petals.PetalsÂ¶

class langchain.llms.petals.Petals[source]Â¶
Bases: LLM
Petals Bloom models.
To use, you should have the petals python package installed, and the
environment variable HUGGINGFACE_API_KEY set with your API key.
Any parameters that are valid to be passed to the call can be passed
in, even if not explicitly saved on this class.
Example
from langchain.llms import petals
petals = Petals()

Create a new model by parsing and validating input data from keyword arguments.
Raises ValidationError if the input data cannot be parsed to form a valid model.

param cache: Optional[bool] = NoneÂ¶

param callback_manager: Optional[BaseCallbackManager] = NoneÂ¶

param callbacks: Callbacks = NoneÂ¶

param client: Any = NoneÂ¶
The client to use for the API calls.

param do_sample: bool = TrueÂ¶
Whether or not to use sampling; use greedy decoding otherwise.

param huggingface_api_key: Optional[str] = NoneÂ¶

param max_length: Optional[int] = NoneÂ¶
The maximum length of the sequence to be generated.

param max_new_tokens: int = 256Â¶
The maximum number of new tokens to generate in the completion.

param metadata: Optional[Dict[str, Any]] = NoneÂ¶
Metadata to add to the run trace.

param model_kwargs: Dict[str, Any] [Optional]Â¶
Holds any model parameters valid for create call
not explicitly specified.

param model_name: str = 'bigscience/bloom-petals'Â¶
The model to use.

param tags: Optional[List[str]] = NoneÂ¶
Tags to add to the run trace.

param temperature: float = 0.7Â¶
What sampling temperature to use

param tokenizer: Any = NoneÂ¶
The tokenizer to use for the API calls.

param top_k: Optional[int] = NoneÂ¶
The number of highest probability vocabulary tokens
to keep for top-k-filtering.

param top_p: float = 0.9Â¶
The cumulative probability for top-p sampling.

param verbose: bool [Optional]Â¶
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) â†’ strÂ¶
Check Cache and run the LLM on the given prompt and input.

async abatch(inputs: List[Union[PromptValue, str, List[BaseMessage]]], config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None, *, return_exceptions: bool = False, **kwargs: Any) â†’ List[str]Â¶
Default implementation runs ainvoke in parallel using asyncio.gather.
The default implementation of batch works well for IO bound runnables.
Subclasses should override this method if they can batch more efficiently;
e.g., if the underlying runnable uses an API which supports a batch mode.

async agenerate(prompts: List[str], stop: Optional[List[str]] = None, callbacks: Union[List[BaseCallbackHandler], BaseCallbackManager, None, List[Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]]] = None, *, tags: Optional[Union[List[str], List[List[str]]]] = None, metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None, run_name: Optional[Union[str, List[str]]] = None, **kwargs: Any) â†’ LLMResultÂ¶
Run the LLM on the given prompt and input.

async agenerate_prompt(prompts: List[PromptValue], stop: Optional[List[str]] = None, callbacks: Union[List[BaseCallbackHandler], BaseCallbackManager, None, List[Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]]] = None, **kwargs: Any) â†’ LLMResultÂ¶
Asynchronously pass a sequence of prompts and return model generations.
This method should make use of batched calls for models that expose a batched
API.

Use this method when you want to:
take advantage of batched calls,
need more output from the model than just the top generated value,

are building chains that are agnostic to the underlying language modeltype (e.g., pure text completion models vs chat models).

Parameters

prompts â€“ List of PromptValues. A PromptValue is an object that can be
converted to match the format of any language model (string for pure
text generation models and BaseMessages for chat models).
stop â€“ Stop words to use when generating. Model output is cut off at the
first occurrence of any of these substrings.
callbacks â€“ Callbacks to pass through. Used for executing additional
functionality, such as logging or streaming, throughout generation.
**kwargs â€“ Arbitrary additional keyword arguments. These are usually passed
to the model provider API call.

Returns

An LLMResult, which contains a list of candidate Generations for each inputprompt and additional model provider-specific output.</doc_content>
</document>





<document index='2'>
<source>https://python.langchain.com/docs/integrations/llms/</source>
<doc_content>LLMs | ğŸ¦œï¸ğŸ”— Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# LLMs

## Features (natively supported)â€‹

All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. `ainvoke`, `batch`, `abatch`, `stream`, `astream`. This gives all LLMs basic support for async, streaming and batch, which by default is implemented as below:

- _Async_ support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread.

- _Streaming_ support defaults to returning an `Iterator` (or `AsyncIterator` in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations.

- _Batch_ support defaults to calling the underlying LLM in parallel for each input by making use of a thread pool executor (in the sync batch case) or `asyncio.gather` (in the async batch case). The concurrency can be controlled with the `max_concurrency` key in `RunnableConfig`.

Each LLM integration can optionally provide native implementations for async, streaming or batch, which, for providers that support it, can be more efficient. The table shows, for each integration, which features have been implemented with native support.

| Model | Invoke | Async invoke | Stream | Async stream | Batch | Async batch |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| AI21 | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| AlephAlpha | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| AmazonAPIGateway | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Anthropic | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |
| Anyscale | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Arcee | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Aviary | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| AzureMLOnlineEndpoint | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| AzureOpenAI | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Banana | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Baseten | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Beam | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Bedrock | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ |
| CTransformers | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| CTranslate2 | âœ… | âŒ | âŒ | âŒ | âœ… | âŒ |
| CerebriumAI | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| ChatGLM | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Clarifai | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Cohere | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Databricks | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| DeepInfra | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |
| DeepSparse | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |
| EdenAI | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Fireworks | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| ForefrontAI | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| GPT4All | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| GigaChat | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| GooglePalm | âœ… | âŒ | âŒ | âŒ | âœ… | âŒ |
| GooseAI | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| GradientLLM | âœ… | âœ… | âŒ | âŒ | âœ… | âœ… |
| HuggingFaceEndpoint | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| HuggingFaceHub | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| HuggingFacePipeline | âœ… | âŒ | âŒ | âŒ | âœ… | âŒ |
| HuggingFaceTextGenInference | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |
| HumanInputLLM | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| JavelinAIGateway | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| KoboldApiLLM | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| LlamaCpp | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ |
| ManifestWrapper | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Minimax | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| MlflowAIGateway | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Modal | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| MosaicML | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| NIBittensorLLM | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| NLPCloud | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Nebula | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| OctoAIEndpoint | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Ollama | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| OpaquePrompts | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| OpenAI | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| OpenLLM | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| OpenLM | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| PaiEasEndpoint | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ |
| Petals | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| PipelineAI | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Predibase | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| PredictionGuard | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| PromptLayerOpenAI | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| QianfanLLMEndpoint | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |
| RWKV | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Replicate | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ |
| SagemakerEndpoint | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| SelfHostedHuggingFaceLLM | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| SelfHostedPipeline | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| StochasticAI | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| TextGen | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| TitanTakeoff | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ |
| TitanTakeoffPro | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ |
| Tongyi | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| VLLM | âœ… | âŒ | âŒ | âŒ | âœ… | âŒ |
| VLLMOpenAI | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| VertexAI | âœ… | âœ… | âœ… | âŒ | âœ… | âœ… |
| VertexAIModelGarden | âœ… | âœ… | âŒ | âŒ | âœ… | âœ… |
| Writer | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Xinference | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| YandexGPT | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |

- [Features (natively supported)](#features-natively-supported)</doc_content>
</document>





<document index='3'>
<source>https://python.langchain.com/docs/integrations/llms/petals</source>
<doc_content>Petals | ğŸ¦œï¸ğŸ”— Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# Petals

`Petals` runs 100B+ language models at home, BitTorrent-style.

This notebook goes over how to use Langchain with [Petals](https://github.com/bigscience-workshop/petals).

## Install petalsâ€‹

The `petals` package is required to use the Petals API. Install `petals` using `pip3 install petals`.

For Apple Silicon(M1/M2) users please follow this guide [https://github.com/bigscience-workshop/petals/issues/147#issuecomment-1365379642](https://github.com/bigscience-workshop/petals/issues/147#issuecomment-1365379642) to install petals 

```bash
pip3 install petals
```

## Importsâ€‹

```python
import os

from langchain.chains import LLMChain
from langchain.llms import Petals
from langchain.prompts import PromptTemplate
```

## Set the Environment API Keyâ€‹

Make sure to get [your API key](https://huggingface.co/docs/api-inference/quicktour#get-your-api-token) from Huggingface.

```python
from getpass import getpass

HUGGINGFACE_API_KEY = getpass()
```

```text
     Â·Â·Â·Â·Â·Â·Â·Â·
```

```python
os.environ["HUGGINGFACE_API_KEY"] = HUGGINGFACE_API_KEY
```

## Create the Petals instanceâ€‹

You can specify different parameters such as the model name, max new tokens, temperature, etc.

```python
# this can take several minutes to download big files!

llm = Petals(model_name="bigscience/bloom-petals")
```

```text
    Downloading:   1%|â–                        | 40.8M/7.19G [00:24<15:44, 7.57MB/s]
```

## Create a Prompt Templateâ€‹

We will create a prompt template for Question and Answer.

```python
template = """Question: {question}

Answer: Let's think step by step."""

prompt = PromptTemplate(template=template, input_variables=["question"])
```

## Initiate the LLMChainâ€‹

```python
llm_chain = LLMChain(prompt=prompt, llm=llm)
```

## Run the LLMChainâ€‹

Provide a question and run the LLMChain.

```python
question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"

llm_chain.run(question)
```

- [Install petals](#install-petals)

- [Imports](#imports)

- [Set the Environment API Key](#set-the-environment-api-key)

- [Create the Petals instance](#create-the-petals-instance)

- [Create a Prompt Template](#create-a-prompt-template)

- [Initiate the LLMChain](#initiate-the-llmchain)

- [Run the LLMChain](#run-the-llmchain)</doc_content>
</document>





</documents>



Answer: According to the retrieved documents, there is no mention of "stream" being natively supported by Petals LLM. The documents only provide information on how to use Langchain with Petals and do not mention native support for streams.