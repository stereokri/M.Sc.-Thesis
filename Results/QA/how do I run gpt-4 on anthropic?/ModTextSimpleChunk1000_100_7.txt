<documents>
<document index='0'>
<source>https://python.langchain.com/docs/expression_language/how_to/routing</source>
<doc_content>media or subscribe to their newsletter to stay up to date on new features and releases.\n\n- For most people, the easiest way to leverage Anthropic's technology is through their website - just create an account to get started!", additional_kwargs={}, example=False)</doc_content>
</document>





<document index='1'>
<source>https://python.langchain.com/docs/guides/evaluation/trajectory/trajectory_eval</source>
<doc_content>## Configuring the Evaluation LLM‚Äã

If you don't select an LLM to use for evaluation, the [load_evaluator](https://api.python.langchain.com/en/latest/evaluation/langchain.evaluation.loading.load_evaluator.html#langchain.evaluation.loading.load_evaluator) function will use `gpt-4` to power the evaluation chain. You can select any chat model for the agent trajectory evaluator as below.

```python
# %pip install anthropic
# ANTHROPIC_API_KEY=<YOUR ANTHROPIC API KEY>
```

```python
from langchain.chat_models import ChatAnthropic

eval_llm = ChatAnthropic(temperature=0)
evaluator = load_evaluator("trajectory", llm=eval_llm)
```

```python
evaluation_result = evaluator.evaluate_agent_trajectory(
    prediction=result["output"],
    input=result["input"],
    agent_trajectory=result["intermediate_steps"],
)
evaluation_result
```</doc_content>
</document>





<document index='2'>
<source>https://python.langchain.com/docs/integrations/providers/gpt4all</source>
<doc_content>GPT4All | ü¶úÔ∏èüîó Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# GPT4All

This page covers how to use the `GPT4All` wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.

## Installation and Setup‚Äã

- Install the Python package with `pip install pyllamacpp`

- Download a [GPT4All model](https://github.com/nomic-ai/pyllamacpp#supported-model) and place it in your desired directory

## Usage‚Äã

### GPT4All‚Äã

To use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model's configuration.

```python
from langchain.llms import GPT4All

# Instantiate the model. Callbacks support token-wise streaming
model = GPT4All(model="./models/gpt4all-model.bin", n_ctx=512, n_threads=8)

# Generate text
response = model("Once upon a time, ")
```

You can also customize the generation parameters, such as n_predict, temp, top_p, top_k, and others.</doc_content>
</document>





<document index='3'>
<source>https://python.langchain.com/docs/guides/evaluation/string/criteria_eval_chain</source>
<doc_content>## Configuring the LLM‚Äã

If you don't specify an eval LLM, the `load_evaluator` method will initialize a `gpt-4` LLM to power the grading chain. Below, use an anthropic model instead.

```python
# %pip install ChatAnthropic
# %env ANTHROPIC_API_KEY=<API_KEY>
```

```python
from langchain.chat_models import ChatAnthropic

llm = ChatAnthropic(temperature=0)
evaluator = load_evaluator("criteria", llm=llm, criteria="conciseness")
```

```python
eval_result = evaluator.evaluate_strings(
    prediction="What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.",
    input="What's 2+2?",
)
print(eval_result)
```</doc_content>
</document>





<document index='4'>
<source>https://python.langchain.com/docs/expression_language/how_to/message_history</source>
<doc_content>```bash
pip install -U langchain redis anthropic
```

Set your [Anthropic API  key](https://console.anthropic.com/):

```python
import getpass
import os

os.environ["ANTHROPIC_API_KEY"] = getpass.getpass()
```

Start a local Redis Stack server if we don't have an existing Redis deployment to connect to:

```bash
docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest
```

```python
REDIS_URL = "redis://localhost:6379/0"
```

### LangSmith‚Äã

LangSmith is especially useful for something like message history injection, where it can be hard to otherwise understand what the inputs are to various parts of the chain.

Note that LangSmith is not needed, but it is helpful.
If you do want to use LangSmith, after you sign up at the link above, make sure to uncoment the below and set your environment variables to start logging traces:

```python
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```</doc_content>
</document>





<document index='5'>
<source>https://python.langchain.com/docs/templates/extraction-anthropic-functions</source>
<doc_content>And add the following code to your `server.py` file:

```python
from extraction_anthropic_functions import chain as extraction_anthropic_functions_chain

add_routes(app, extraction_anthropic_functions_chain, path="/extraction-anthropic-functions")
```

(Optional) Let's now configure LangSmith.
LangSmith will help us trace, monitor and debug LangChain applications.
LangSmith is currently in private beta, you can sign up [here](https://smith.langchain.com/).
If you don't have access, you can skip this section

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>
export LANGCHAIN_PROJECT=<your-project>  # if not specified, defaults to "default"
```

If you are inside this directory, then you can spin up a LangServe instance directly by:

```shell
langchain serve
```

This will start the FastAPI app with a server is running locally at
[http://localhost:8000](http://localhost:8000)</doc_content>
</document>





<document index='6'>
<source>https://python.langchain.com/docs/templates/anthropic-iterative-search</source>
<doc_content>And add the following code to your `server.py` file:

```python
from anthropic_iterative_search import chain as anthropic_iterative_search_chain

add_routes(app, anthropic_iterative_search_chain, path="/anthropic-iterative-search")
```

(Optional) Let's now configure LangSmith.
LangSmith will help us trace, monitor and debug LangChain applications.
LangSmith is currently in private beta, you can sign up [here](https://smith.langchain.com/).
If you don't have access, you can skip this section

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=<your-api-key>
export LANGCHAIN_PROJECT=<your-project>  # if not specified, defaults to "default"
```

If you are inside this directory, then you can spin up a LangServe instance directly by:

```shell
langchain serve
```

This will start the FastAPI app with a server is running locally at
[http://localhost:8000](http://localhost:8000)</doc_content>
</document>





</documents>



Answer: To run GPT-4 on Anthropic, you can use the `ChatAnthropic` model from LangChain. You'll need to install the required package and set your Anthrop API key as an environment variable. Then, you can instantiate the `ChatAnthropic` model and use it for evaluation or other tasks.