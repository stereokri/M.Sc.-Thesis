<documents>
<document index='0'>
<source>https://python.langchain.com/docs/guides/local_llms</source>
<doc_content>```python
pip install gpt4all
```

```python
from langchain.llms import GPT4All

llm = GPT4All(
    model="/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin"
)
```

```python
llm("The first man on the moon was ... Let's think step by step")
```

```text
    ".\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these"
```

## Prompts‚Äã

Some LLMs will benefit from specific prompts.

For example, LLaMA will use [special tokens](https://twitter.com/RLanceMartin/status/1681879318493003776?s=20).

We can use `ConditionalPromptSelector` to set prompt based on the model type.

```python
# Set our LLM
llm = LlamaCpp(
    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",
    n_gpu_layers=1,
    n_batch=512,
    n_ctx=2048,
    f16_kv=True,
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
    verbose=True,
)
```

Set the associated prompt based upon the model version.

```python
from langchain.chains import LLMChain
from langchain.chains.prompt_selector import ConditionalPromptSelector
from langchain.prompts import PromptTemplate

DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""<<SYS>> \n You are an assistant tasked with improving Google search \
results. \n <</SYS>> \n\n [INST] Generate THREE Google search queries that \
are similar to this question. The output should be a numbered list of questions \
and each should have a question mark at the end: \n\n {question} [/INST]""",
)

DEFAULT_SEARCH_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""You are an assistant tasked with improving Google search \
results. Generate THREE Google search queries that are similar to \
this question. The output should be a numbered list of questions and each \
should have a question mark at the end: {question}""",
)

QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(
    default_prompt=DEFAULT_SEARCH_PROMPT,
    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],
)</doc_content>
</document>





<document index='1'>
<source>https://python.langchain.com/docs/integrations/providers/gpt4all</source>
<doc_content>GPT4All | ü¶úÔ∏èüîó Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# GPT4All

This page covers how to use the `GPT4All` wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.

## Installation and Setup‚Äã

- Install the Python package with `pip install pyllamacpp`

- Download a [GPT4All model](https://github.com/nomic-ai/pyllamacpp#supported-model) and place it in your desired directory

## Usage‚Äã

### GPT4All‚Äã

To use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model's configuration.

```python
from langchain.llms import GPT4All

# Instantiate the model. Callbacks support token-wise streaming
model = GPT4All(model="./models/gpt4all-model.bin", n_ctx=512, n_threads=8)

# Generate text
response = model("Once upon a time, ")
```

You can also customize the generation parameters, such as n_predict, temp, top_p, top_k, and others.

To stream the model's predictions, add in a CallbackManager.

```python
from langchain.llms import GPT4All
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# There are many CallbackHandlers supported, such as
# from langchain.callbacks.streamlit import StreamlitCallbackHandler

callbacks = [StreamingStdOutCallbackHandler()]
model = GPT4All(model="./models/gpt4all-model.bin", n_ctx=512, n_threads=8)

# Generate text. Tokens are streamed through the callback manager.
model("Once upon a time, ", callbacks=callbacks)
```

## Model File‚Äã

You can find links to model file downloads in the [pyllamacpp](https://github.com/nomic-ai/pyllamacpp) repository.

For a more detailed walkthrough of this, see [this notebook](/docs/integrations/llms/gpt4all)

- [Installation and Setup](#installation-and-setup)

- [Usage](#usage)- [GPT4All](#gpt4all-1)

- [Model File](#model-file)</doc_content>
</document>





<document index='2'>
<source>https://python.langchain.com/docs/integrations/text_embedding/gpt4all</source>
<doc_content>GPT4All | ü¶úÔ∏èüîó Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# GPT4All

[GPT4All](https://gpt4all.io/index.html) is a free-to-use, locally running, privacy-aware chatbot. There is no GPU or internet required. It features popular models and its own models such as GPT4All Falcon, Wizard, etc.

This notebook explains how to use [GPT4All embeddings](https://docs.gpt4all.io/gpt4all_python_embedding.html#gpt4all.gpt4all.Embed4All) with LangChain.

## Install GPT4All's Python Bindings‚Äã

```python
%pip install gpt4all > /dev/null
```

Note: you may need to restart the kernel to use updated packages.

```python
from langchain.embeddings import GPT4AllEmbeddings
```

```python
gpt4all_embd = GPT4AllEmbeddings()
```

```text
    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45.5M/45.5M [00:02<00:00, 18.5MiB/s]

    Model downloaded at:  /Users/rlm/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin

    objc[45711]: Class GGMLMetalClass is implemented in both /Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x29fe18208) and /Users/rlm/anaconda3/envs/lcn2/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x2a0244208). One of the two will be used. Which one is undefined.
```

```python
text = "This is a test document."
```

## Embed the Textual Data‚Äã

```python
query_result = gpt4all_embd.embed_query(text)
```

With embed_documents you can embed multiple pieces of text. You can also map these embeddings with [Nomic's Atlas](https://docs.nomic.ai/index.html) to see a visual representation of your data.

```python
doc_result = gpt4all_embd.embed_documents([text])
```

- [Install GPT4All's Python Bindings](#install-gpt4alls-python-bindings)

- [Embed the Textual Data](#embed-the-textual-data)</doc_content>
</document>





<document index='3'>
<source>https://python.langchain.com/docs/integrations/chat/anthropic_functions</source>
<doc_content>Anthropic Functions | ü¶úÔ∏èüîó Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# Anthropic Functions

This notebook shows how to use an experimental wrapper around Anthropic that gives it the same API as OpenAI Functions.

```python
from langchain_experimental.llms.anthropic_functions import AnthropicFunctions
```

```text
    /Users/harrisonchase/.pyenv/versions/3.9.1/envs/langchain/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.
      warnings.warn(
```

## Initialize Model‚Äã

You can initialize this wrapper the same way you'd initialize ChatAnthropic

```python
model = AnthropicFunctions(model="claude-2")
```

## Passing in functions‚Äã

You can now pass in functions in a similar way

```python
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g. San Francisco, CA",
                },
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
        },
    }
]
```

```python
from langchain.schema import HumanMessage
```

```python
response = model.predict_messages(
    [HumanMessage(content="whats the weater in boston?")], functions=functions
)
```

```python
response
```

```text
    AIMessage(content=' ', additional_kwargs={'function_call': {'name': 'get_current_weather', 'arguments': '{"location": "Boston, MA", "unit": "fahrenheit"}'}}, example=False)
```

## Using for extraction‚Äã

You can now use this for extraction.

```python
from langchain.chains import create_extraction_chain

schema = {
    "properties": {
        "name": {"type": "string"},
        "height": {"type": "integer"},
        "hair_color": {"type": "string"},
    },
    "required": ["name", "height"],
}
inp = """
Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.
        """
```

```python
chain = create_extraction_chain(schema, model)
```

```python
chain.run(inp)
```

```text
    [{'name': 'Alex', 'height': '5', 'hair_color': 'blonde'},
     {'name': 'Claudia', 'height': '6', 'hair_color': 'brunette'}]
```

## Using for tagging‚Äã

You can now use this for tagging

```python
from langchain.chains import create_tagging_chain
```

```python
schema = {
    "properties": {
        "sentiment": {"type": "string"},
        "aggressiveness": {"type": "integer"},
        "language": {"type": "string"},
    }
}
```

```python
chain = create_tagging_chain(schema, model)
```

```python
chain.run("this is really cool")
```</doc_content>
</document>





<document index='4'>
<source>https://python.langchain.com/docs/integrations/llms/gpt4all</source>
<doc_content>GPT4All | ü¶úÔ∏èüîó Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# GPT4All

[GitHub:nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.

This example goes over how to use LangChain to interact with `GPT4All` models.

```python
%pip install gpt4all > /dev/null
```

```text
    Note: you may need to restart the kernel to use updated packages.
```

### Import GPT4All‚Äã

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import LLMChain
from langchain.llms import GPT4All
from langchain.prompts import PromptTemplate
```

### Set Up Question to pass to LLM‚Äã

```python
template = """Question: {question}

Answer: Let's think step by step."""

prompt = PromptTemplate(template=template, input_variables=["question"])
```

### Specify Model‚Äã

To run locally, download a compatible ggml-formatted model. 

The [gpt4all page](https://gpt4all.io/index.html) has a useful `Model Explorer` section:

- Select a model of interest

- Download using the UI and move the `.bin` to the `local_path` (noted below)

For more info, visit [https://github.com/nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all).

```python
local_path = (
    "./models/ggml-gpt4all-l13b-snoozy.bin"  # replace with your desired local file path
)
```

```python
# Callbacks support token-wise streaming
callbacks = [StreamingStdOutCallbackHandler()]

# Verbose is required to pass to the callback manager
llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)

# If you want to use a custom model add the backend parameter
# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends
llm = GPT4All(model=local_path, backend="gptj", callbacks=callbacks, verbose=True)
```

```python
llm_chain = LLMChain(prompt=prompt, llm=llm)
```

```python
question = "What NFL team won the Super Bowl in the year Justin Bieber was born?"

llm_chain.run(question)
```

Justin Bieber was born on March 1, 1994. In 1994, The Cowboys won Super Bowl XXVIII.

- [Import GPT4All](#import-gpt4all)

- [Set Up Question to pass to LLM](#set-up-question-to-pass-to-llm)

- [Specify Model](#specify-model)</doc_content>
</document>





<document index='5'>
<source>https://api.python.langchain.com/en/latest/llms/langchain.llms.anthropic.Anthropic.html</source>
<doc_content>langchain.llms.anthropic.Anthropic ‚Äî ü¶úüîó LangChain 0.0.337

API

Experimental

Python Docs

Toggle Menu

PrevUp
Next

LangChain 0.0.337

langchain.llms.anthropic.Anthropic

langchain.llms.anthropic.Anthropic¬∂

class langchain.llms.anthropic.Anthropic[source]¬∂
Bases: LLM, _AnthropicCommon
Anthropic large language models.
To use, you should have the anthropic python package installed, and the
environment variable ANTHROPIC_API_KEY set with your API key, or pass
it as a named parameter to the constructor.
Example
import anthropic
from langchain.llms import Anthropic

model = Anthropic(model="<model_name>", anthropic_api_key="my-api-key")

# Simplest invocation, automatically wrapped with HUMAN_PROMPT
# and AI_PROMPT.
response = model("What are the biggest risks facing humanity?")

# Or if you want to use the chat mode, build a few-shot-prompt, or
# put words in the Assistant's mouth, use HUMAN_PROMPT and AI_PROMPT:
raw_prompt = "What are the biggest risks facing humanity?"
prompt = f"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}"
response = model(prompt)

Create a new model by parsing and validating input data from keyword arguments.
Raises ValidationError if the input data cannot be parsed to form a valid model.

param AI_PROMPT: Optional[str] = None¬∂

param HUMAN_PROMPT: Optional[str] = None¬∂

param anthropic_api_key: Optional[SecretStr] = None¬∂

Constraints

type = string
writeOnly = True
format = password

param anthropic_api_url: Optional[str] = None¬∂

param cache: Optional[bool] = None¬∂

param callback_manager: Optional[BaseCallbackManager] = None¬∂

param callbacks: Callbacks = None¬∂

param count_tokens: Optional[Callable[[str], int]] = None¬∂

param default_request_timeout: Optional[float] = None¬∂
Timeout for requests to Anthropic Completion API. Default is 600 seconds.

param max_tokens_to_sample: int = 256 (alias 'max_tokens')¬∂
Denotes the number of tokens to predict per generation.

param metadata: Optional[Dict[str, Any]] = None¬∂
Metadata to add to the run trace.

param model: str = 'claude-2' (alias 'model_name')¬∂
Model name to use.

param model_kwargs: Dict[str, Any] [Optional]¬∂

param streaming: bool = False¬∂
Whether to stream the results.

param tags: Optional[List[str]] = None¬∂
Tags to add to the run trace.

param temperature: Optional[float] = None¬∂
A non-negative float that tunes the degree of randomness in generation.

param top_k: Optional[int] = None¬∂
Number of most likely tokens to consider at each step.

param top_p: Optional[float] = None¬∂
Total probability mass of tokens to consider at each step.

param verbose: bool [Optional]¬∂
Whether to print out response text.

__call__(prompt: str, stop: Optional[List[str]] = None, callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) ‚Üí str¬∂
Check Cache and run the LLM on the given prompt and input.</doc_content>
</document>





<document index='6'>
<source>https://python.langchain.com/docs/integrations/chat/anthropic</source>
<doc_content>Anthropic | ü¶úÔ∏èüîó Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# Anthropic

This notebook covers how to get started with Anthropic chat models.

```python
from langchain.chat_models import ChatAnthropic
from langchain.schema import HumanMessage
```

```python
chat = ChatAnthropic()
```

```python
messages = [
    HumanMessage(
        content="Translate this sentence from English to French. I love programming."
    )
]
chat(messages)
```

```text
    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)
```

## ChatAnthropic also supports async and streaming functionality:‚Äã

```python
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
```

```python
await chat.agenerate([messages])
```

```text
    LLMResult(generations=[[ChatGeneration(text=" J'aime programmer.", generation_info=None, message=AIMessage(content=" J'aime programmer.", additional_kwargs={}, example=False))]], llm_output={}, run=[RunInfo(run_id=UUID('8cc8fb68-1c35-439c-96a0-695036a93652'))])
```

```python
chat = ChatAnthropic(
    streaming=True,
    verbose=True,
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
)
chat(messages)
```

```text
     J'aime la programmation.

    AIMessage(content=" J'aime la programmation.", additional_kwargs={}, example=False)
```

- [ChatAnthropic also supports async and streaming functionality:](#chatanthropic-also-supports-async-and-streaming-functionality)</doc_content>
</document>





</documents>



Answer: To run GPT-4 on Anthropic, you can create a new model by parsing and validating input data from keyword arguments. For example, `model = ChatAnthropic()` and then `response = model("What are the biggest risks facing humanity?")`. This will automatically wrap the prompt with HUMAN_PROMPT and AI_PROMPT.