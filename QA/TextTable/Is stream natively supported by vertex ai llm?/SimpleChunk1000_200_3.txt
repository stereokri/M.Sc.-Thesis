<documents>
<document index='0'>
<source>/home/kristian/BenchMarkingPythonScripts/AdditionalDocs/NativeSupport.txt</source>
<doc_content>- _Streaming_ support defaults to returning an `Iterator` (or `AsyncIterator` in the case of async streaming) of a single value, the final result returned by the 
underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that 
expects an iterator of tokens can work for any of our LLM integrations.

- _Batch_ support defaults to calling the underlying LLM in parallel for each input by making use of a thread pool executor (in the sync batch case) or `asyncio.gather` 
(in the async batch case). The concurrency can be controlled with the `max_concurrency` key in `RunnableConfig`.

Each LLM integration can optionally provide native implementations for async, streaming or batch, which, for providers that support it, can be more efficient. The table 
shows, for each integration, which features have been implemented with native support.</doc_content>
</document>





<document index='1'>
<source>/home/kristian/BenchMarkingPythonScripts/AdditionalDocs/NativeSupport.txt</source>
<doc_content>LLMs | ğŸ¦œï¸ğŸ”— Langchain

[Skip to main content](#docusaurus_skipToContent_fallback)# LLMs

## Features (natively supported)â€‹

All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. `ainvoke`, `batch`, `abatch`, `stream`, `astream`. This gives all 
LLMs basic support for async, streaming and batch, which by default is implemented as below:

- _Async_ support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make 
progress while the LLM is being executed, by moving this call to a background thread.</doc_content>
</document>





<document index='2'>
<source>/home/kristian/BenchMarkingPythonScripts/AdditionalDocs/NativeSupport.txt</source>
<doc_content>| Model | Invoke | Async invoke | Stream | Async stream | Batch | Async batch |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| AI21 | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| AlephAlpha | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| AmazonAPIGateway | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Anthropic | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |
| Anyscale | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Arcee | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Aviary | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| AzureMLOnlineEndpoint | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| AzureOpenAI | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| Banana | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Baseten | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Beam | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Bedrock | âœ… | âŒ | âœ… | âŒ | âŒ | âŒ |
| CTransformers | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| CTranslate2 | âœ… | âŒ | âŒ | âŒ | âœ… | âŒ |
| CerebriumAI | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| ChatGLM | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Clarifai | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| Cohere | âœ… | âœ… | âŒ | âŒ | âŒ | âŒ |
| Databricks | âœ… | âŒ | âŒ | âŒ | âŒ | âŒ |
| DeepInfra | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |
| DeepSparse | âœ… | âœ… | âœ… | âœ… | âŒ | âŒ |</doc_content>
</document>





</documents>



Answer: The documents do not mention "Vertex AI LLM" as one of the models. Therefore, it is not possible to determine if stream is natively supported by Vertex AI LLM based on the provided information.